[TOC]

# 机器学习

## 概论 #1

- 经验
  - 把反映数据内在规律的信息叫做特征
  - 无监督学习：没有标记/目标，因此也无法从事预测事务，更适合对数据结构的分析
  - 监督学习：有标记/目标
- 性能：评价所完成的任务质量的指标
  - 测试集：具备相同特征的数据
  - 使用测试集与模型的预测结果进行对比
    - 出现在测试集中的样本一定不能被用于模型训练
  - 性能衡量标准
    - 准确率
    - 偏差大小
- 结果：
  - 二类分类器：使用一条直线来划分A类和B类，斜率和截距为决定这条直线的因素。



[TOC]

## 基础 #2

### 2.1监督学习经典模型

- 分类学习-回归预测 概述
  1. 准备训练数据
  2. 抽取所需特征，形成特征向量
  3. 把特征向量连同对应目标送入学习算法中，训练出一个预测模型
  4. 采用同样的特征抽取方法得到用于测试的特征向量
  5. 使用预测模型对这些用于测试的特征向量进行预测并获得结果
  
- 模型的选用

  ![](https://scikit-learn.org/stable/_static/ml_map.png)

#### 分类学习

###### 线性分类器

- 是一种假设特征与分类结果存在线性关系的模型

- 逻辑斯蒂回归模型

  - 逻辑斯蒂函数：将f 映射到 (0,1)

    $g(z)=\frac{1}{1+e^{-z}}$

  - 线性关系$f(w,x,b)=w^{T}x+b$

  $h_{w,b}(x)=g(f(w,x,b))=\frac{1}{1+e^{-f}}=\frac{1}{1+e^{-(W^{T}x+b)}}$

- 最大似然估计的概率L(w,b)

  $L(w,b)=\pi^{m}_{i=1}(h_{w,b}(i))^{y^{i}}(1-h_{w,b}(i))^{1-y^{i}}$

- 随机梯度上升算法：学习到决定模型的参数：w,b

  `例：02Linear.py良恶性乳腺癌肿瘤预测`

###### 支持向量机(分类)器Support Vector Classifier

> 在多个训练样本表现良好的分类模型中寻找一个最好的

- 在两个空间间隔最小的两个不同类别的数据点叫做“支持向量”，他们可以用于决策最优线性分类模型（相对最优）

- 能够提高识别性能

  `例：03SVC.py使用支持向量机处理手写图片数据集`

###### 朴素贝叶斯

- 构造基础：贝叶斯理论，假设各个维度上的特征被分类的条件概率之间是相互独立的

  `例：04Bayes.py`

###### K近邻（分类）

- 以距离为量度标准，不需要训练参数
- 决策算法  

###### 决策树

> 很多场景下，学习特征和目标之间不遵照线性假设。如：相比于老人和小孩，青壮年更不易因流感而死亡

- 所有叶子节点显示模型的决策结果

- 使用决策树模型描述非线性关系

  `例：05DecisionTreeClassifier.py`

###### 集成模型（分类）

- 集成分类模型是综合考量多个分类器的预测结果，从而做出决策。方式分为两种
  - 01》利用相同的训练数据同时搭建多个独立的分类模型，然后通过投票的方式，以少数服从多数的原则作出最终分类决策
  - 02》按照一定次序搭建多个分类模型，这些模型之间彼此存在依赖关系。一般而言，每一个后续模型的加入都需要对现有集成模型的综合考量有所贡献，进而不断提升更新过后的集成模型的性能，搭建出具有更强分类能力的模型

[TOC]

#### 回归预测

> 回归问题和分类问题的区别在于：期待预测的目标是连续变量

###### 线性回归器

- `例：06Regression.py`

- 当数据量大的时候选用SGDR更高效

- 在不清楚特征之间关系的情况下，可以使用线性回归模型作为大多数科学试验的基线系统（baseline system）

###### 支持向量机（回归）

- 从训练数据中选取一部分更加有效的支持向量
- 配置不同的核函数来改变模型性能
- `例：06Regression.py`

###### K近邻（回归）

- `例：06b_KNR.py`

- 和K近邻（分类）一样，属于无参数模型

- 相比于K近邻（分类），衍生出了衡量样本回归值的不同方式：使用普通的算术平均算法，还是同时考虑距离的差异进行加权平均

  - 配置两种模型

    ```python
    uni_knr = KNeighborsRegressor(weights='uniform')
    
    dis_knr = KNeighborsRegressor(weights='distance')
    ```

###### 回归树

- 思路与决策树类似，选择不同特征作为分裂节点
- 不同之处：回归树叶结点的数据类型不是离散型，而是连续型
- 优点
  - 树模型可以解决非线性特征问题
  - 树模型不要求对特征标准化和统一量化，即数据型和类别型特征都可以直接被应用在树模型的构建和预测过程中
  - 因此，树模型可以直观地输出决策过程，使得预测结果具有可解释性
- 缺点
  - 正因为树模型可以解决复杂的非线性拟合问题，所以更加容易因为模型搭建过于复杂而丧失对新预测数据的精度（泛化力）
  - 树模型从上至下的预测过程会因为数据细微的更改而发生较大的结构变化，因此预测稳定性较差
  - 依托训练数据构建最佳的树模型是NP难问题，即在有限时间内无法找到最优解的问题。因此使用类似贪婪算法的解法只能找到一些次优解。因此，经常借用集成模型在多个次优解中寻觅更高的模型性能

###### 集成模型（回归）

- 普通随机森林
- 提升树模型
- 极端随机森林
  - 与普通随机森林不同的是，极端随机森林在每当构建一棵树的分裂节点时，不会任意地选取特征，而是先随机收集一部分特征，然后利用信息熵和基尼不纯性等指标挑选最佳的节点特征
- 具有更高的表现性能和更好的稳定性



[TOC]

### 2.2无监督学习经典模型

- 无监督学习着重于发现数据本身的分布特点。与监督学习不同，无监督学习不需要对数据进行标记。在节省人工的同时也可以利用数据规模变得不可限量
- 从功能角度讲，无监督学习模型易于发现数据的“群落”，同时也可以寻找“离群”的样本
- 对于特征维度非常高的数据样本，同样可以通过无监督学习对数据进行降维，保留最具有区分性的低维度特征

#### 数据聚类--K均值算法

- 数据聚类中最经典、相对容易理解的模型
- 算法4个阶段
  1. 随机布设K个特征空间内的点作为初始的聚类中心
  2. 对于根据每个数据的特征向量，从K个聚类中心中寻找距离最近的一个，并且把该数据标记为从属于这个聚类中心
  3. 在所有的数据都被标记过聚类中心后，根据这些数据新分配的类簇，重新对K个聚类中心做计算
  4. 如果一轮下来，所有的数据点从属的聚类中心与上一次的类簇没有变化，则迭代可停止，否则从②重新开始
- `例：07kMeans.py`
  - 前者使用ARI（Adjusted Rand Index）来评估
  - 后者（两个图）使用轮廓系数（Silhouette Coefficient）来度量聚类结果的质量。如果被用于评估的数据没有所属类别，使用此方法效果会更好。轮廓系数同时兼顾了聚类的凝聚度和分离度
  - 可以从代码结果的图中获知当聚类中心数量k为3时轮廓系数最大
- 缺点
  - 容易收敛到局部最优解
    - 可以通过执行多次K-means算法来挑选性能最好的初始中心点
  - 需要预先设定簇的数量
- 肘部观察法——粗略地预估相对合理的类簇个数
  - 通过观察所有数据点到其所属类簇距离的平方和 随着 K的走势来找出最佳的类簇数量
  - 通常拐点（下降速率变化点）是相对最佳的类簇数量
  - `例：08elbow.py`

#### 特征降维--主成分分析PCA

- 特征降维是无监督学习的另一个应用，目的有二
  - 经常会在实际项目中遭遇特征维度非常高的训练样本，而又无法借助自己的领域知识人工构建有效特征
  - 在数据表现方面，无法用肉眼观察三个维度的数据特征
- 数据降维重构了有效的低维度特征向量，同时也为数据展现提供了可能

- 主成分分析PCA，是最为经典和实用的特征降维技术，特别在辅助图像识别方面有突出的表现
  - `例：09PCA.py`            待补充
  - 降维/压缩问题是选取数据中具有代表性的特征。。。。

[TOC]

## 进阶 #3

### 3.1 模型使用技巧

##### 特征提升

> 早期机器学习的研究与应用受模型种类和运算能力的限制。因此大部分研发人员把更多的精力放在对数据的预处理上

- 特征抽取
- 特征筛选

##### 模型正则化

- 欠拟合与过拟合
- L~1~范数正则化

##### 模型检验

- 留一验证
- 交叉验证

##### 超参数搜索

- 网格搜索
- 并行搜索

### 3.2 流行库、模型实践

##### 自然语言处理包

##### 词向量技术

##### XGBoost模型

##### Tensorflow框架



[TOC]

## 实战 #4

